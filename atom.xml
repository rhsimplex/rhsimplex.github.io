<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>R Henderson</title>
  <icon>https://rhsimplex.github.io/images/favicon-192x192.png</icon>
  
  <link href="https://rhsimplex.github.io/atom.xml" rel="self"/>
  
  <link href="https://rhsimplex.github.io/"/>
  <updated>2022-05-09T08:09:31.742Z</updated>
  <id>https://rhsimplex.github.io/</id>
  
  <author>
    <name>Ryan Henderson</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SageMaker vs Vertex in early 2022</title>
    <link href="https://rhsimplex.github.io/2022/05/05/sagemaker-vs-vertex/"/>
    <id>https://rhsimplex.github.io/2022/05/05/sagemaker-vs-vertex/</id>
    <published>2022-05-05T15:38:43.000Z</published>
    <updated>2022-05-09T08:09:31.742Z</updated>
    
    <content type="html"><![CDATA[<p>To ensure, ahem, <em>efficient</em> usage of <a href="/2022/04/26/cloud-credit-where-credit-is-due/" title="startup cloud credits">startup cloud credits</a>, I’ve recently completed a machine learning platform migration from Google’s <a href="https://cloud.google.com/vertex-ai">VertexAI</a>, to Amazon’s <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html">SageMaker</a>. I want to share my limited impressions about each platform.</p><h2 id="Which-is-better"><a href="#Which-is-better" class="headerlink" title="Which is better?"></a>Which is better?</h2><p>VertexAI is clunkier to use, but feels more “correct” in a software-engineering sense. SageMaker is more straight-forward for a technical user without much software engineering background – the archetypical data scientist – but has some anti-patterns that make integration a bit more treacherous.</p><h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><p>I did the migration in early 2022.   My use case is training neural networks, and in particular hyperparameter optimization. I have a CI pipeline (outside of AWS or GCP) that builds and tests the model code and pushes a container image to the platforms’ respective container registries. Most of the model code is PyTorch.</p><p>I don’t want to pretend to have covered everything these platforms offer, which is a lot. To summarize I have, a bit of experience on each platform with:</p><ul><li>Containerized Training </li><li>Hyperparameter searches</li><li>Jupyter notebooks</li><li>S3&#x2F;GCS integration</li></ul><p>And a non-exhaustive list of things I haven’t used yet:</p><ul><li>Model deployment&#x2F;lifecycle</li><li>Data pipelines</li><li>Auto ML</li></ul><h2 id="What-they-have-in-common"><a href="#What-they-have-in-common" class="headerlink" title="What they have in common"></a>What they have in common</h2><p>I haven’t worked with cloud ML platforms in a few years, and I’m happy to say they’ve come a long way. Both SageMaker and VertexAI do what I expected with only minor headaches. Both apply a cost markup to compute resources used for training (e.g. the training instance used on SageMaker will cost more per hour than the same instance on EC2), and unfortunately it’s not at clear what percentage this is or if it’s stable over time. VertexAI has a slight edge here because you can choose your accelerator, i.e. GPU, separately from the base instance. In SageMaker, they’re lumped together.</p><p>One surprising annoyance is the UI for either can be very slow–surprisingly, frustratingly slow. I try to use the CLI or write my own scripts with the respective SDKs, but I would expect one of the advantages of using a platform like SageMaker or VertexAI would be the slick, informative UI. Sadly, this isn’t the case. </p><h2 id="How-much-will-I-need-to-change-my-training-code"><a href="#How-much-will-I-need-to-change-my-training-code" class="headerlink" title="How much will I need to change my training code?"></a>How much will I need to change my training code?</h2><p>In an ideal world, you should not have to modify your training code at all to integrate it into a machine learning platform. This is not the case with SageMaker or VertexAI, but SageMaker is a bit worse in this respect. </p><p>If you’ve written and containerized a training script, you probably have a Dockerfile that ends with something like:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;python&quot;</span>, <span class="string">&quot;train.py&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>That way, arguments (hyperparameters) you pass to the container are passed to the script directly. VertexAI does this by default. I’ve since learned that Sagemaker <a href="https://github.com/aws/sagemaker-training-toolkit">can also do this</a>, but that wasn’t initially obvious to me from the sprawling documentation. By default, Sagemaker drops a <code>hyperparameters.json</code> into your container while sending its own arguments to the container. So if you don’t want to add another dependency to your build, you might have to add something like this before your <code>train.py</code> script:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">if</span> sys.argv[<span class="number">1</span>] == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        <span class="comment"># You are training withing Amazon Sagemaker, which inserts this `train` argument at the beginning.</span></span><br><span class="line">        <span class="comment"># You&#x27;ll need to extract hyperparameters from a JSON file. This is a hack, but Sagemaker doesn&#x27;t</span></span><br><span class="line">        <span class="comment"># provide a way to pass arguments to the container, as far as I know.</span></span><br><span class="line">        extracted_command_line_args = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/opt/ml/input/config/hyperparameters.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> read_file:</span><br><span class="line">            parameters = json.load(read_file)</span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> parameters.items():</span><br><span class="line">                <span class="keyword">if</span> key == <span class="string">&#x27;_tuning_objective_metric&#x27;</span>:</span><br><span class="line">                    <span class="comment"># Another parameter inserted by SageMaker which can be ignored</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> value == <span class="string">&#x27;NULL&#x27;</span>:</span><br><span class="line">                    <span class="comment"># Some parameters don&#x27;t take an argument. Sagemaker can&#x27;t really handle this.</span></span><br><span class="line">                    <span class="comment"># so you could pass it into AWS environments like --toggle_something=NULL. The &#x27;NULL&#x27; will be dropped.</span></span><br><span class="line">                    extracted_command_line_args.append(key)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                extracted_command_line_args.append(<span class="string">f&#x27;<span class="subst">&#123;key&#125;</span>=<span class="subst">&#123;value&#125;</span>&#x27;</span>)</span><br><span class="line">        args = parse_args(sys.argv[<span class="number">2</span>:] + extracted_command_line_args)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        args = parse_args(sys.argv[<span class="number">1</span>:])</span><br><span class="line">    train(args)</span><br></pre></td></tr></table></figure><p>Another pitfall this example doesn’t show is that while the Python <code>ArgumentParser</code> is capable of handling repeated arguments to make a sequence (for example: <code>--dataset dogs --dataset cats --dataset raccoons</code> could give you a <code>dataset: [dogs, cats, raccoons]</code>), since SageMaker send hyperparameters around in key value pairs in a JSON (or through a <code>dict</code> if you’re using the Python SDK), you can’t use this pattern.</p><p>Both platforms will mount a storage volume (S3 or GCS bucket) with your training data. So here you can either supply your training script with these paths or if you’re lazy like me, just copy everything to the directory you expect it to be.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.isdir(<span class="string">&#x27;/gcs/your-bucket-name/&#x27;</span>):</span><br><span class="line">    <span class="comment"># you&#x27;re on Vertex </span></span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">elif</span> os.path.isdir(<span class="string">&#x27;/opt/ml/input/data/&#x27;</span>):</span><br><span class="line">    <span class="comment"># you&#x27;re on SageMaker </span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>Similarly, both platform provide a mounted directory to send model artifacts to (like checkpoints and logs). Both S3 and GCS have nice <code>rsync</code>-like functionality so you can download what you need after training. SageMaker compresses everything in your artifact directory before storing it. This sounds like a good idea, but is very annoying if you just want to download, say, the Tensorboard logs without a massive model checkpoint. SageMaker also doesn’t allow you to SSH into a running training container as far as I can tell, and VertexAI does (but it’s not enabled by default).</p><h2 id="Hyperparameter-tuning"><a href="#Hyperparameter-tuning" class="headerlink" title="Hyperparameter tuning"></a>Hyperparameter tuning</h2><p>If you’re tuning hyperparameters, you need some way to send validation results from individual runs back to the tuner. SageMaker handles this elegantly by letting you provide a regular expression that it uses to parse the logs. This means you don’t need to change your training code at all as long as you’re logging the value you want to tune. Vertex requires your training code to send values to the tuner directly, for example with its <a href="https://github.com/GoogleCloudPlatform/cloudml-hypertune">cloudlml-hypertune</a> package. This isn’t a big deal, but it feels unnecessary to include another dependency for this.</p><p>The Vertex HPS supports <a href="https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview">conditional hyperparemeters</a> and SageMaker, as far as I can see, does not. A conditional hyperparameter just means the hyperparameter will only be tuned if some condition is met. For instance, you may want to tune models on an L2 or smooth L1 loss, and in the latter case you would want to also tune the alpha parameter. In SageMaker this is not possible, and the alpha parameter will be “tuned” even when using L2 loss.</p><p>Neither supports <a href="https://optuna.readthedocs.io/en/v1.0.0/tutorial/pruning.html">pruning jobs</a> that I can see, which is a shame because it could potentially save you a lot of money.</p><h2 id="Tensorboard-integration"><a href="#Tensorboard-integration" class="headerlink" title="Tensorboard integration"></a>Tensorboard integration</h2><p>Vertex AI has a nice tensorboard integration that is <a href="https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-training">kind of a pain</a> to set up but works pretty well once it’s running. Just make sure you configure the mysterious service account correctly. Oh, and I hope you’re ok with the <a href="https://cloud.google.com/vertex-ai/pricing#tensorboard"><strong>absurd $300 per user per month cost</strong></a>.</p><p>Even though Tensorboard can generate PR curves, the VertexAI version mysteriously doesn’t support them (as of January 2022). Of course, I’m only assuming that’s the case since my PR-curves render correctly when serving Tensorboard locally. I couldn’t find any information about which version of Tensorboard the VertexAI platform runs or any differences between the VertexAI vs. vanilla Tensorboard. Considering Tensorboard is created by Google, this is pretty puzzling.</p><p>SageMaker <a href="https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html?highlight=tensorboard#capture-real-time-tensorboard-data-from-the-debugging-hook">has a Tensorboard integration as well</a>, but I have not managed to get this working yet. I will update this section if I ever do. Tensorboard can take an <a href="https://stackoverflow.com/questions/47425882/tensorboard-logdir-with-s3-path">S3 URI directly</a> which is pretty cool, but as I mentioned above, SageMaker is compressing your model artifacts, so this doesn’t work out of the box.</p><h2 id="Notebooks"><a href="#Notebooks" class="headerlink" title="Notebooks"></a>Notebooks</h2><p>Both platforms use a customized JupyterLab setup for notebooks, so you will hopefully be on familiar territory here. Notably, the VertexAI instances seem to persist the conda environments between sessions, while SageMaker resets them. Both are valid design choices in my opinion.  If you want to run some setup code for SageMaker to recreate your environment, you can <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html">easily do so</a>.</p><p>However, I couldn’t find an elegant way to integrate an external Git repository in either case. I ended up adding an SSH key to each notebook instance on which I wanted to pull code with Git.</p><h2 id="GUI"><a href="#GUI" class="headerlink" title="GUI"></a>GUI</h2><p>This will largely be a matter of taste, but the AWS UI is in general more polished and logical than GCP. Browsing logs in particular seems weirdly complicated on GCP. Also filtering (e.g. training jobs, of which you may have thousands) in Vertex only works from the beginning of the name while the SageMaker does a full-text search. As I said above, both GUIs are pretty uninspired.</p><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>The platform-level docs for both platforms are a bit overwhelming but complete. If anything SageMaker has too much documentation to search through effectively. There are probably <a href="https://github.com/aws/amazon-sagemaker-examples">thousands of Jupyter notebooks</a> that show you how to do everything you could possibly want. I personally don’t like having to search through these as documentation, but I appreciate the commitment to providing working examples for nearly every functionality. </p><p>The SDK documentation <a href="https://sagemaker.readthedocs.io/en/stable/">is a lot better for SageMaker</a>; someone actually cared to organize this and make it readable. The <a href="https://googleapis.dev/python/aiplatform/latest/aiplatform.html">VertexAI counterpart</a> is probably autogenerated and will require some work to get your bearings.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Both VertexAI and SageMaker are much more than MLOps platforms, which can be a blessing for some but a curse for others. They seem to be trying to cover every ML use-case and every kind of user, which can add friction if you have a very specific idea of what you want to accomplish.</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;p&gt;To ensure, ahem, &lt;em&gt;efficient&lt;/em&gt; usage of &lt;a href=&quot;/2022/04/26/cloud-credit-where-credit-is-due/&quot; title=&quot;startup cloud</summary>
        
      
    
    
    
    
    <category term="mlops" scheme="https://rhsimplex.github.io/tags/mlops/"/>
    
    <category term="aws" scheme="https://rhsimplex.github.io/tags/aws/"/>
    
    <category term="gcp" scheme="https://rhsimplex.github.io/tags/gcp/"/>
    
  </entry>
  
  <entry>
    <title>The Cloud Credit Trap</title>
    <link href="https://rhsimplex.github.io/2022/04/26/cloud-credit-where-credit-is-due/"/>
    <id>https://rhsimplex.github.io/2022/04/26/cloud-credit-where-credit-is-due/</id>
    <published>2022-04-26T15:07:27.000Z</published>
    <updated>2022-05-09T08:09:31.742Z</updated>
    
    <content type="html"><![CDATA[<p>I’m a big fan of <a href="https://github.com/PyTorchLightning/pytorch-lightning">PyTorch Lightning</a> and use it <a href="https://github.com/bayer-science-for-a-better-life/molnet-geometric-lightning">whenever I can</a>. So I was excited to hear the Lightning creators were launching their own MLOps platform <a href="grid.ai">GridAI</a>. Unfortunately, through no fault of its creators, I will probably never use it.</p><h2 id="The-credit-trap"><a href="#The-credit-trap" class="headerlink" title="The credit trap"></a>The credit trap</h2><p>If I’m working at a big company and need MLOps, it’s unlikely that I’ll get permission to sign up for GridAI. I’ll probably be forced to use SageMaker or VertexAI, the AWS and GCP equivalents respectively. Fair enough – the company is big and established and so are the cloud providers they’re already using.</p><p>What’s counter-intuitive is that I’ll probably get the same outcome at a startup. Why? The cloud credit trap: that well-known Faustian lock-in every startup CTO will take to get their company $50-100k of free cloud stuff. Why on earth would they authorize me to start spending hundreds or thousands of dollars on GridAI when I could be spending zeros of dollars on Sagemaker for the “same thing?” They’ve traded curiosity&#x2F;risk for expedience, and are right to do so: I would do the same!</p><figure>  <img src="/images/maxresdefault.jpg" alt="A skeptical tech lead listens intently to an AWS Startup Solutions Architect"/>  <figcaption style="text-align: center"><em>A tech lead listens intently to an AWS Startup Solutions Architect</em></figcaption></figure><h2 id="The-irony"><a href="#The-irony" class="headerlink" title="The irony"></a>The irony</h2><p>GridAI is <a href="https://www.grid.ai/billing-rates/">hosted on AWS</a>. But you can’t use those AWS cloud credits on GridAI.</p><p>It’s understandable. Imagine the nightmare of trying to apply AWS credits through billing from a third party. A cloud provider has no incentive to do that, especially if they want to push you towards <em>their</em> managed offering. </p><p>I’m not the first to point out this monopolistic quagmire. I just find this example – a company giving you free money to get hooked on their product while also collecting cloud rent from their competitor’s product – particularly twisted.</p><p>It’s hard to see a world where AWS or GCP would let cloud credits flow through to services built on their infrastructure. But one could imagine a less managed-service-oriented competitor explicitly encouraging this as a way to grow their overall customer base.</p><p>So who is the target user for something like GridAI? I want it to be me, but due to the cloud credit trap it’s hard to imagine a professional situation where I’d ever use it.</p>]]></content>
    
    
      
      
        
        
    <summary type="html">&lt;p&gt;I’m a big fan of &lt;a href=&quot;https://github.com/PyTorchLightning/pytorch-lightning&quot;&gt;PyTorch Lightning&lt;/a&gt; and use it &lt;a</summary>
        
      
    
    
    
    
    <category term="ml" scheme="https://rhsimplex.github.io/tags/ml/"/>
    
    <category term="mlops" scheme="https://rhsimplex.github.io/tags/mlops/"/>
    
    <category term="aws" scheme="https://rhsimplex.github.io/tags/aws/"/>
    
  </entry>
  
</feed>
